{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g0mhHj9bR10"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "7g0mhHj9bR10"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "890bb3c7"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from operator import itemgetter\n",
        "from itertools import combinations\n",
        "import time\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "import scipy.io as sio\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import pickle"
      ],
      "id": "890bb3c7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78ce2004"
      },
      "outputs": [],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "id": "78ce2004"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "924ddd00"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'      #Indra: device set to 0\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "tf.compat.v1.disable_eager_execution()  # Indra: Disable eager execution for compat\n",
        "\n",
        "np.random.seed(0)"
      ],
      "id": "924ddd00"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e5e92f"
      },
      "source": [
        "# DATA LOADING"
      ],
      "id": "06e5e92f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6043fa0c"
      },
      "outputs": [],
      "source": [
        "#LOAD Disease Gene Adjacency Network\n",
        "gene_phenes_path = '/content/drive/MyDrive/data_prioritization/genes_phenes.mat'\n",
        "f = h5py.File(gene_phenes_path, 'r')\n",
        "gene_network_adj = sp.csc_matrix((np.array(f['GeneGene_Hs']['data']),\n",
        "    np.array(f['GeneGene_Hs']['ir']), np.array(f['GeneGene_Hs']['jc'])),\n",
        "    shape=(12331,12331))\n",
        "gene_network_adj = gene_network_adj.tocsr()"
      ],
      "id": "6043fa0c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae15f0f5"
      },
      "outputs": [],
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape"
      ],
      "id": "ae15f0f5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "875c7bc4"
      },
      "outputs": [],
      "source": [
        "def network_edge_threshold(network_adj, threshold):\n",
        "    edge_tmp, edge_value, shape_tmp = sparse_to_tuple(network_adj)\n",
        "    preserved_edge_index = np.where(edge_value>threshold)[0]\n",
        "    preserved_network = sp.csr_matrix(\n",
        "        (edge_value[preserved_edge_index],\n",
        "        (edge_tmp[preserved_edge_index,0], edge_tmp[preserved_edge_index, 1])),\n",
        "        shape=shape_tmp)\n",
        "    return preserved_network"
      ],
      "id": "875c7bc4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc999264"
      },
      "outputs": [],
      "source": [
        "disease_network_adj = sp.csc_matrix((np.array(f['PhenotypeSimilarities']['data']),\n",
        "    np.array(f['PhenotypeSimilarities']['ir']), np.array(f['PhenotypeSimilarities']['jc'])),\n",
        "    shape=(3215, 3215))\n",
        "disease_network_adj = disease_network_adj.tocsr()  ## CONVERTED TO SPARSE ROW\n",
        "# >0.2 values get preserved\n",
        "disease_network_adj = network_edge_threshold(disease_network_adj, 0.2)"
      ],
      "id": "cc999264"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75a032a2"
      },
      "outputs": [],
      "source": [
        "## NOTE: Gene Disease Adj is 1, 0 matrix, while gene_gene (12331, 12331)\n",
        "##        and disease-disease ( 3215, 3215) have edge level values\n",
        "\n",
        "dg_ref = f['GenePhene'][0][0]\n",
        "gene_disease_adj = sp.csc_matrix((np.array(f[dg_ref]['data']),\n",
        "    np.array(f[dg_ref]['ir']), np.array(f[dg_ref]['jc'])),\n",
        "    shape=(12331, 3215))\n",
        "gene_disease_adj = gene_disease_adj.tocsr()"
      ],
      "id": "75a032a2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1fe014e"
      },
      "outputs": [],
      "source": [
        "## WHAT DATA is THIS ? 34 novel associtaions of value 1, 0\n",
        "novel_associations_adj = sp.csc_matrix((np.array(f['NovelAssociations']['data']),\n",
        "    np.array(f['NovelAssociations']['ir']), np.array(f['NovelAssociations']['jc'])),\n",
        "    shape=(12331,3215))\n"
      ],
      "id": "b1fe014e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9faec36"
      },
      "outputs": [],
      "source": [
        "#Feature value of each Gene: 4536 features per gene\n",
        "gene_feature_path = '/content/drive/MyDrive/data_prioritization/GeneFeatures.mat'\n",
        "f_gene_feature = h5py.File(gene_feature_path,'r')\n",
        "gene_feature_exp = np.array(f_gene_feature['GeneFeatures'])\n",
        "gene_feature_exp = np.transpose(gene_feature_exp)\n",
        "gene_network_exp = sp.csc_matrix(gene_feature_exp)"
      ],
      "id": "b9faec36"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d2cfb7b",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# // TODO: Explore why 1 - 9 and not 0 - 9\n",
        "## NOTE THIS DATA is different from gene_disease_adj\n",
        "\n",
        "row_list = [3215, 1137, 744, 2503, 1143, 324, 1188, 4662, 1243]\n",
        "gene_feature_list_other_spe = list()\n",
        "for i in range(1,9):\n",
        "    dg_ref = f['GenePhene'][i][0]\n",
        "    disease_gene_adj_tmp = sp.csc_matrix((np.array(f[dg_ref]['data']),\n",
        "        np.array(f[dg_ref]['ir']), np.array(f[dg_ref]['jc'])),\n",
        "        shape=(12331, row_list[i]))\n",
        "    gene_feature_list_other_spe.append(disease_gene_adj_tmp)"
      ],
      "id": "8d2cfb7b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26646523",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "disease_tfidf_path = '/content/drive/MyDrive/data_prioritization/clinicalfeatures_tfidf.mat'\n",
        "f_disease_tfidf = h5py.File(disease_tfidf_path)\n",
        "disease_tfidf = np.array(f_disease_tfidf['F'])\n",
        "disease_tfidf = np.transpose(disease_tfidf)\n",
        "disease_tfidf = sp.csc_matrix(disease_tfidf)"
      ],
      "id": "26646523"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67482211"
      },
      "outputs": [],
      "source": [
        "dis_dis_adj_list= list()\n",
        "dis_dis_adj_list.append(disease_network_adj)"
      ],
      "id": "67482211"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6657a5"
      },
      "source": [
        "# DATA PARAMS"
      ],
      "id": "ba6657a5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b13a917f"
      },
      "outputs": [],
      "source": [
        "val_test_size = 0.1\n",
        "n_genes = 12331\n",
        "n_dis = 3215\n",
        "n_dis_rel_types = len(dis_dis_adj_list)\n",
        "gene_adj = gene_network_adj\n",
        "gene_degrees = np.array(gene_adj.sum(axis=0)).squeeze()\n",
        "gene_degrees.shape"
      ],
      "id": "b13a917f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c75d1c8"
      },
      "outputs": [],
      "source": [
        "gene_dis_adj = gene_disease_adj\n",
        "dis_gene_adj = gene_dis_adj.transpose(copy=True)\n",
        "dis_degrees_list = [np.array(dis_adj.sum(axis=0)).squeeze() for dis_adj in dis_dis_adj_list]\n",
        "dis_degrees_list[0].shape"
      ],
      "id": "9c75d1c8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96af076f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "adj_mats_orig = {\n",
        "    (0, 0): [gene_adj, gene_adj.transpose(copy=True)],\n",
        "    (0, 1): [gene_dis_adj],\n",
        "    (1, 0): [dis_gene_adj],\n",
        "    (1, 1): dis_dis_adj_list + [x.transpose(copy=True) for x in dis_dis_adj_list],\n",
        "}\n",
        "degrees = {\n",
        "    0: [gene_degrees, gene_degrees],\n",
        "    1: dis_degrees_list + dis_degrees_list,\n",
        "}"
      ],
      "id": "96af076f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "000198df"
      },
      "outputs": [],
      "source": [
        "# Gene Feature Exp - Feature vector  + other 8 species\n",
        "gene_feat = sp.hstack(gene_feature_list_other_spe+[gene_feature_exp])\n",
        "gene_nonzero_feat, gene_num_feat = gene_feat.shape\n",
        "gene_feat = sparse_to_tuple(gene_feat.tocoo())\n",
        "\n",
        "dis_feat = disease_tfidf\n",
        "dis_nonzero_feat, dis_num_feat = dis_feat.shape\n",
        "dis_feat = sparse_to_tuple(dis_feat.tocoo())\n"
      ],
      "id": "000198df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5265eb1"
      },
      "outputs": [],
      "source": [
        "num_feat = {\n",
        "    0: gene_num_feat,\n",
        "    1: dis_num_feat,\n",
        "}\n",
        "nonzero_feat = {\n",
        "    0: gene_nonzero_feat,\n",
        "    1: dis_nonzero_feat,\n",
        "}\n",
        "feat = {\n",
        "    0: gene_feat,\n",
        "    1: dis_feat,\n",
        "}"
      ],
      "id": "a5265eb1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "933d2589"
      },
      "outputs": [],
      "source": [
        "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
        "# edge_type2decoder = {\n",
        "#     (0, 0): 'bilinear',\n",
        "#     (0, 1): 'bilinear',\n",
        "#     (1, 0): 'bilinear',\n",
        "#     (1, 1): 'bilinear',\n",
        "# }\n",
        "\n",
        "edge_type2decoder = {\n",
        "    (0, 0): 'innerproduct',\n",
        "    (0, 1): 'innerproduct',\n",
        "    (1, 0): 'innerproduct',\n",
        "    (1, 1): 'innerproduct',\n",
        "}\n",
        "\n",
        "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
        "num_edge_types = sum(edge_types.values())\n",
        "print(\"Edge types:\", \"%d\" % num_edge_types)"
      ],
      "id": "933d2589"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fbc5c07"
      },
      "source": [
        "# INSIDE MAIN CALLING CODE"
      ],
      "id": "8fbc5c07"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "603abbc6"
      },
      "source": [
        "# FLAGS"
      ],
      "id": "603abbc6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytAfuMaBlcNA"
      },
      "source": [
        "By Indra: Converted all flags to variables"
      ],
      "id": "ytAfuMaBlcNA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fc5a7ae",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "neg_sample_size =  1  #Negative sample size\n",
        "learning_rate =  0.01 # 'Initial learning rate\n",
        "hidden1_units =  64 #'Number of units in hidden layer 1\n",
        "hidden2_units = 32 #Number of units in hidden layer 2\n",
        "weight_decay =  0.001  #Weight for L2 loss on embedding matrix\n",
        "dropout = 0.1 # Dropout rate (1 - keep probability)\n",
        "max_margin =  0.1 #Max margin parameter in hinge loss\n",
        "batch_size = 512\n",
        "bias = True #Bias term"
      ],
      "id": "0fc5a7ae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc39102f"
      },
      "outputs": [],
      "source": [
        "def construct_placeholders(edge_types):\n",
        "    placeholders = {\n",
        "        'batch': tf.compat.v1.placeholder(tf.int32, name='batch'),\n",
        "        'batch_edge_type_idx': tf.compat.v1.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
        "        'batch_row_edge_type': tf.compat.v1.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
        "        'batch_col_edge_type': tf.compat.v1.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
        "        'degrees': tf.compat.v1.placeholder(tf.int32),\n",
        "        'dropout': tf.compat.v1.placeholder_with_default(0., shape=()),\n",
        "        'neg_sample_indexes': tf.compat.v1.placeholder(tf.int32),\n",
        "    }\n",
        "    placeholders.update({\n",
        "        'adj_mats_%d,%d,%d' % (i, j, k): tf.compat.v1.sparse_placeholder(tf.float32)\n",
        "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
        "    placeholders.update({\n",
        "        'feat_%d' % i: tf.compat.v1.sparse_placeholder(tf.float32)\n",
        "        for i, _ in edge_types})\n",
        "    return placeholders"
      ],
      "id": "fc39102f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5678f65a"
      },
      "outputs": [],
      "source": [
        "print(\"Defining placeholders\")\n",
        "placeholders = construct_placeholders(edge_types)"
      ],
      "id": "5678f65a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoB4_pomt_ac"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "epoch_losses_history = defaultdict(list,)\n",
        "current_epoch_loss = [[] for _ in range(num_edge_types)]"
      ],
      "id": "FoB4_pomt_ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78e51508"
      },
      "source": [
        "# Defining Minibatches"
      ],
      "id": "78e51508"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cfeb741"
      },
      "outputs": [],
      "source": [
        "class EdgeMinibatchIterator(object):\n",
        "    \"\"\" This minibatch iterator iterates over batches of sampled edges or\n",
        "    random pairs of co-occuring edges.\n",
        "    assoc -- numpy array with target edges\n",
        "    placeholders -- tensorflow placeholders object\n",
        "    batch_size -- size of the minibatches\n",
        "    \"\"\"\n",
        "    def __init__(self, adj_mats, feat, edge_types, batch_size=100, val_test_size=0.01):\n",
        "        self.adj_mats = adj_mats  #Cell 26\n",
        "        self.feat = feat           #{ 0: [12331, 17480 feature matrix], 1: [3215, 16592 feature matrix] }\n",
        "        self.edge_types = edge_types     #{(0, 0): 2, (0, 1): 1, (1, 0): 1, (1, 1): 2}\n",
        "        self.batch_size = batch_size         #512\n",
        "        self.val_test_size = val_test_size       #0.1\n",
        "        self.num_edge_types = sum(self.edge_types.values())        #6\n",
        "\n",
        "        self.iter = 0\n",
        "        self.freebatch_edge_types= list(range(self.num_edge_types))            #{[0,1,2,3,4,5]}\n",
        "        self.batch_num = [0]*self.num_edge_types        #[0,0,0,0,0,0]\n",
        "        self.current_edge_type_idx = 0\n",
        "        self.edge_type2idx = {}\n",
        "        self.idx2edge_type = {}\n",
        "        r = 0\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                self.edge_type2idx[i, j, k] = r          #{(0,0,0): 0, (0,0,1): 1 ...}\n",
        "                self.idx2edge_type[r] = i, j, k          #{0: (0,0,0), 1: (0,0,1) ...}\n",
        "                r += 1\n",
        "\n",
        "        self.train_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()} #{(0,0):[None, None], ..}\n",
        "        self.val_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.test_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "        self.val_edges_false = {edge_type: [None]*n for edge_type, n in self.edge_types.items()}\n",
        "\n",
        "        # Function to build test and val sets with val_test_size positive links\n",
        "        self.adj_train = {edge_type: [None]*n for edge_type, n in self.edge_types.items()} #{(0,0):[None, None], ..}\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                self.mask_test_edges((i, j), k)\n",
        "\n",
        "    def mask_test_edges(self, edge_type, type_idx):\n",
        "        edges_all, _, _ = sparse_to_tuple(self.adj_mats[edge_type][type_idx])  #Cell 26 convert to {(n*2: x,y),n*val,shape} Cell 40\n",
        "        num_test = max(100, int(np.floor(edges_all.shape[0] * self.val_test_size)))  #edges_all are the coords in adj matrix\n",
        "        num_val = max(100, int(np.floor(edges_all.shape[0] * self.val_test_size)))   #num_test is max 100 ??? WHYYYY??\n",
        "\n",
        "        if edge_type not in [(0,1), (1, 0)]:   # DIS_DIS and GENE_GENE only 10 ???? WHYY??\n",
        "            num_test = 10\n",
        "            num_val = 10\n",
        "\n",
        "        all_edge_idx = list(range(edges_all.shape[0]))  #edges_all.shape[0] - > Num of non_zero values in sparse [0,1 .. N]\n",
        "        np.random.shuffle(all_edge_idx)\n",
        "\n",
        "        val_edge_idx = all_edge_idx[:num_val]\n",
        "        val_edges = edges_all[val_edge_idx]\n",
        "\n",
        "        test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "        test_edges = edges_all[test_edge_idx]\n",
        "\n",
        "        train_edges = np.delete(edges_all, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "\n",
        "        #Checks if edges are part of edges_all, then adds to test_edges_false if not there, same length as test_edges\n",
        "        test_edges_false = []\n",
        "        while len(test_edges_false) < len(test_edges):\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if test_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], test_edges_false):\n",
        "                    continue\n",
        "            test_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        val_edges_false = []\n",
        "        while len(val_edges_false) < len(val_edges):\n",
        "            idx_i = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[0])\n",
        "            idx_j = np.random.randint(0, self.adj_mats[edge_type][type_idx].shape[1])\n",
        "            if self._ismember([idx_i, idx_j], edges_all):\n",
        "                continue\n",
        "            if val_edges_false:\n",
        "                if self._ismember([idx_i, idx_j], val_edges_false):\n",
        "                    continue\n",
        "            val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "        #ALL _edges and _edges_false are arrays of coordinates [n*[x,y]]\n",
        "        # Re-build adj matrices\n",
        "        data = np.ones(train_edges.shape[0])\n",
        "        adj_train = sp.csr_matrix(\n",
        "            (data, (train_edges[:, 0], train_edges[:, 1])),\n",
        "            shape=self.adj_mats[edge_type][type_idx].shape)        #adj_train are now just 1's\n",
        "        self.adj_train[edge_type][type_idx] = self.preprocess_graph(adj_train)\n",
        "\n",
        "        self.train_edges[edge_type][type_idx] = train_edges\n",
        "        self.val_edges[edge_type][type_idx] = val_edges\n",
        "        self.val_edges_false[edge_type][type_idx] = np.array(val_edges_false)\n",
        "        self.test_edges[edge_type][type_idx] = test_edges\n",
        "        self.test_edges_false[edge_type][type_idx] = np.array(test_edges_false)\n",
        "\n",
        "    def _ismember(self, a, b):\n",
        "        a = np.array(a)\n",
        "        b = np.array(b)\n",
        "        rows_close = np.all(a - b == 0, axis=1)  #Subtracts the pair a(idx_i, idx_j) from each element of b, if equal then\n",
        "        return np.any(rows_close)                #Function will return true if found a coordinates\n",
        "\n",
        "    # Symteric (D^-0.5 . (A + I) . D^-0.5) and Assymteric (Dr ^ -0.5 . A . Dc ^ -0.5) normalization\n",
        "    # For A(mxm) -> adj_ = A + I(mxm), R(mx1) = Sum of rows of adj_, M_inv = D(mxm) (diaginals are roots of R),\n",
        "    # Adj_Normalized = (adj(mxm).D(mxm))'.D(mxm)\n",
        "    #For A(mxn) -> R(mx1) = sum of rows of A, C(nx1) = sum of cols of A, RD(mxm) -> Diagonals(non-null) are roots of R,\n",
        "    # CD(nxn) -> Diagonals(non-null) are roots of C\n",
        "    # Adj_normalized = RD(mxm).A(mxn).CD(nxn)\n",
        "    def preprocess_graph(self, adj):\n",
        "        adj = sp.coo_matrix(adj)\n",
        "        if adj.shape[0] == adj.shape[1]:\n",
        "            adj_ = adj + sp.eye(adj.shape[0])\n",
        "            rowsum = np.array(adj_.sum(1))\n",
        "            degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "            adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "        else:\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "            colsum = np.array(adj.sum(0))\n",
        "            rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
        "            coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n",
        "            adj_normalized = rowdegree_mat_inv.dot(adj).dot(coldegree_mat_inv).tocoo()\n",
        "        return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "    def update_feed_dict(self, feed_dict, dropout, placeholders):\n",
        "        # construct feed dictionary\n",
        "        feed_dict.update({\n",
        "            placeholders['adj_mats_%d,%d,%d' % (i,j,k)]: self.adj_train[i,j][k]\n",
        "            for i, j in self.edge_types for k in range(self.edge_types[i,j])})\n",
        "        feed_dict.update({placeholders['feat_%d' % i]: self.feat[i] for i, _ in self.edge_types})\n",
        "        feed_dict.update({placeholders['dropout']: dropout})\n",
        "        return feed_dict\n",
        "\n",
        "    def batch_feed_dict(self, batch_edges, batch_edge_type, placeholders):\n",
        "        feed_dict = dict()\n",
        "        feed_dict.update({placeholders['batch']: batch_edges})   # List of [x,y] coordinates\n",
        "        feed_dict.update({placeholders['batch_edge_type_idx']: batch_edge_type}) # Type of edge, {0: (0,0,0), 1: (0,0,1) ..}\n",
        "        feed_dict.update({placeholders['batch_row_edge_type']: self.idx2edge_type[batch_edge_type][0]}) # From Edge\n",
        "        feed_dict.update({placeholders['batch_col_edge_type']: self.idx2edge_type[batch_edge_type][1]}) # To Edge\n",
        "        return feed_dict\n",
        "\n",
        "\n",
        "    def next_minibatch_feed_dict(self, placeholders):\n",
        "        \"\"\"Select a random edge type and a batch of edges of the same type\"\"\"\n",
        "\n",
        "        self.current_edge_type_idx = self.iter%len(minibatch.idx2edge_type)\n",
        "        print(\"Current Iteration: \", self.iter, \"Current edge:\", self.current_edge_type_idx)\n",
        "        self.iter += 1\n",
        "\n",
        "        if(len(current_epoch_loss[self.current_edge_type_idx]) > 0 and self.batch_num[self.current_edge_type_idx] == 0): #New Epoch starting, log and average losses\n",
        "            epoch_losses_history[self.current_edge_type_idx].append(np.mean(current_epoch_loss[self.current_edge_type_idx]))\n",
        "            if len(epoch_losses_history[self.current_edge_type_idx]) > 15:\n",
        "                epoch_losses_history[self.current_edge_type_idx].pop(0)\n",
        "            current_epoch_loss[self.current_edge_type_idx] = []\n",
        "\n",
        "\n",
        "        i, j, k = self.idx2edge_type[self.current_edge_type_idx]\n",
        "        print(\"Batch Number: \", self.batch_num[self.current_edge_type_idx])\n",
        "        if self.batch_num[self.current_edge_type_idx] * self.batch_size \\\n",
        "                   > len(self.train_edges[i,j][k]) - self.batch_size + 1:   #Less than batch size elements left\n",
        "            batch_edges = self.train_edges[i,j][k][self.batch_num[self.current_edge_type_idx] * self.batch_size:]\n",
        "            #print(\"Extracting train_edges[\", str(i), \",\",str(j),\"][\",self.batch_num[self.current_edge_type_idx] * self.batch_size,\":]\")\n",
        "            if(batch_edges.shape[0] < 512):\n",
        "                batch_edges = np.concatenate([batch_edges, self.train_edges[i,j][k][0:self.batch_size - batch_edges.shape[0]]], axis = 0)\n",
        "                #print(\"Extracting train_edges[\", str(i), \",\",str(j),\"][0:\",self.batch_size - batch_edges.shape[0],\"]\")\n",
        "            else:\n",
        "                #print(\"Too many elements added, taking only first \", self.batch_size, \" elements\")\n",
        "                batch_edges = batch_edges[0:self.batch_size]\n",
        "            self.batch_num[self.current_edge_type_idx] = 0\n",
        "            assert batch_edges.shape == (self.batch_size,2)\n",
        "            return self.batch_feed_dict(batch_edges, self.current_edge_type_idx, placeholders)\n",
        "\n",
        "\n",
        "\n",
        "        start = self.batch_num[self.current_edge_type_idx] * self.batch_size\n",
        "        self.batch_num[self.current_edge_type_idx] += 1\n",
        "        #print(\"Extracting train_edges[\", str(i), \",\",str(j),\"][\", str(k),\"][\", start, \":\", start + self.batch_size,\"]\")\n",
        "        batch_edges = self.train_edges[i,j][k][start: start + self.batch_size]\n",
        "        assert batch_edges.shape == (self.batch_size,2)\n",
        "        return self.batch_feed_dict(batch_edges, self.current_edge_type_idx, placeholders)\n"
      ],
      "id": "2cfeb741"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5388e096",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Create minibatch iterator\")\n",
        "minibatch = EdgeMinibatchIterator(\n",
        "        adj_mats=adj_mats_orig,  #Check cell 26\n",
        "        feat=feat,              #{ 0: [12331, 17480 feature matrix], 1: [3215, 16592 feature matrix] }\n",
        "        edge_types=edge_types,   #{(0, 0): 2, (0, 1): 1, (1, 0): 1, (1, 1): 2}\n",
        "        batch_size=batch_size,      #512\n",
        "        val_test_size=val_test_size  #0.1\n",
        "    )"
      ],
      "id": "5388e096"
    },
    {
      "cell_type": "code",
      "source": [
        "indices, values, shape = feat[1]"
      ],
      "metadata": {
        "id": "ux23f6QqjVZH"
      },
      "id": "ux23f6QqjVZH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices"
      ],
      "metadata": {
        "id": "22a20NWGjnF7"
      },
      "id": "22a20NWGjnF7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22197b58"
      },
      "source": [
        "##  Defining The Model"
      ],
      "id": "22197b58"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03006503"
      },
      "outputs": [],
      "source": [
        "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
        "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n",
        "    initialization.\n",
        "    \"\"\"\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = tf.compat.v1.random_uniform([input_dim, output_dim], minval=-init_range,\n",
        "                                maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)"
      ],
      "id": "03006503"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5907434e"
      },
      "outputs": [],
      "source": [
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs\n",
        "    \"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "\n",
        "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
        "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements) # TODO: CHANGE THIS\n",
        "    \"\"\"\n",
        "    noise_shape = [num_nonzero_elems]\n",
        "    random_tensor = keep_prob         # 1 - dropout\n",
        "    random_tensor += tf.compat.v1.random_uniform(noise_shape)     #Add 1 - dropout to random matrix of Nx1, N is input\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)    # If > 1, keep or else drop\n",
        "    pre_out = tf.compat.v1.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)                             # The Remaining elements get multiplied, dropout concept\n",
        "\n",
        "\n",
        "class MultiLayer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "\n",
        "    # Properties\n",
        "        name: String, defines the variable scope of the layer.\n",
        "\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "    \"\"\"\n",
        "    def __init__(self, edge_type=(), num_types=-1, **kwargs):\n",
        "        self.edge_type = edge_type\n",
        "        self.num_types = num_types\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.issparse = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            outputs = self._call(inputs)\n",
        "            return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionSparseMulti(MultiLayer):\n",
        "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats,\n",
        "                 nonzero_feat, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionSparseMulti, self).__init__(**kwargs) #input_dim = num_feat:  {0: 17480, 1: 16592}\n",
        "        self.dropout = dropout\n",
        "        self.adj_mats = adj_mats\n",
        "        self.act = act\n",
        "        self.issparse = True\n",
        "        self.nonzero_feat = nonzero_feat  #nonzero_feat:  {0: 12331, 1: 3215}\n",
        "        with tf.compat.v1.variable_scope('%s_vars' % self.name):      #weights_i are glorot variables: input_dimxoutput_dim matrix\n",
        "            for k in range(self.num_types):\n",
        "                print(\"Initializing weights: weights_%d with shape: \" %k, \"And edge type: \", self.edge_type,\"num_types:\", self.num_types)\n",
        "                print(str(input_dim[self.edge_type[1]]) + \" \" + str(output_dim))\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim[self.edge_type[1]], output_dim, name='weights_%d' % k) #input_dim:  {0: 17480, 1: 16592}\n",
        "                                                                                    #output_dim: hidden_1 units = 64\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            x = dropout_sparse(inputs, 1-self.dropout, self.nonzero_feat[self.edge_type[1]]) # x is shape 12331(3215)\n",
        "            x = tf.compat.v1.sparse_tensor_dense_matmul(x, self.vars['weights_%d' % k])       # x 12331 * []\n",
        "            x = tf.compat.v1.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
        "            outputs.append(self.act(x))\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class GraphConvolutionMulti(MultiLayer):\n",
        "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj_mats, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionMulti, self).__init__(**kwargs)\n",
        "        self.adj_mats = adj_mats\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "        with tf.compat.v1.variable_scope('%s_vars' % self.name):\n",
        "            for k in range(self.num_types):\n",
        "                self.vars['weights_%d' % k] = weight_variable_glorot(\n",
        "                    input_dim, output_dim, name='weights_%d' % k)\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            x = tf.nn.dropout(inputs, 1-self.dropout)\n",
        "            x = tf.matmul(x, self.vars['weights_%d' % k])\n",
        "            x = tf.compat.v1.sparse_tensor_dense_matmul(self.adj_mats[self.edge_type][k], x)\n",
        "            outputs.append(self.act(x))\n",
        "        outputs = tf.add_n(outputs)\n",
        "        outputs = tf.nn.l2_normalize(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InnerProductDecoder(MultiLayer):\n",
        "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        i, j = self.edge_type\n",
        "        outputs = []\n",
        "        for k in range(self.num_types):\n",
        "            inputs_row = tf.nn.dropout(inputs[i], 1-self.dropout)\n",
        "            inputs_col = tf.nn.dropout(inputs[j], 1-self.dropout)\n",
        "            rec = tf.matmul(inputs_row, tf.transpose(inputs_col))\n",
        "            outputs.append(self.act(rec))\n",
        "        return outputs\n"
      ],
      "id": "5907434e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2fmyw86dPGi"
      },
      "source": [
        "# Decagon Model"
      ],
      "id": "X2fmyw86dPGi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c9e1e77"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', True)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.compat.v1.variable_scope(self.name):\n",
        "            self._build()\n",
        "        variables = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "    def fit(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class DecagonModel(Model):\n",
        "    def __init__(self, placeholders, num_feat, nonzero_feat, edge_types, decoders, **kwargs):\n",
        "        super(DecagonModel, self).__init__(**kwargs)\n",
        "        self.edge_types = edge_types    #{(0, 0): 2, (0, 1): 1, (1, 0): 1, (1, 1): 2}\n",
        "        self.num_edge_types = sum(self.edge_types.values())      #6\n",
        "        self.num_obj_types = max([i for i, _ in self.edge_types]) + 1       #2\n",
        "        self.decoders = decoders #{(0, 0): 'innerproduct', (0, 1): 'innerproduct', (1, 0): 'innerproduct', (1, 1): 'innerproduct'}\n",
        "        self.inputs = {i: placeholders['feat_%d' % i] for i, _ in self.edge_types} #sparse tensors with n*indices, n*values, shape\n",
        "        self.input_dim = num_feat  #num_feat:  {0: 17480, 1: 16592}\n",
        "        self.nonzero_feat = nonzero_feat #nonzero_feat:  {0: 12331, 1: 3215}\n",
        "        self.placeholders = placeholders\n",
        "        self.dropout = placeholders['dropout']  #default 0\n",
        "        self.adj_mats = {et: [\n",
        "            placeholders['adj_mats_%d,%d,%d' % (et[0], et[1], k)] for k in range(n)]\n",
        "            for et, n in self.edge_types.items()}\n",
        "        self.build()\n",
        "\n",
        "    def _build(self):\n",
        "        self.hidden1 = defaultdict(list)  #Create empty list when trying to access key not there\n",
        "        for i, j in self.edge_types:\n",
        "            self.hidden1[i].append(GraphConvolutionSparseMulti(\n",
        "                input_dim=self.input_dim, output_dim=hidden1_units,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, nonzero_feat=self.nonzero_feat,\n",
        "                act=lambda x: x, dropout=self.dropout,       # no activation ???\n",
        "                logging=self.logging)(self.inputs[j]))      # Logging is only true or false\n",
        "            # self.hidden1[i].append(GraphConvolutionMulti(\n",
        "            #     input_dim=self.input_dim, output_dim=hidden1_units,\n",
        "            #     edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "            #     adj_mats=self.adj_mats, act=lambda x: x,\n",
        "            #     dropout=self.dropout,logging=self.logging)(self.inputs[j]))\n",
        "\n",
        "        for i, hid1 in self.hidden1.items():\n",
        "            self.hidden1[i] = tf.nn.relu(tf.add_n(hid1))\n",
        "\n",
        "        self.embeddings_reltyp = defaultdict(list)\n",
        "        for i, j in self.edge_types:\n",
        "            self.embeddings_reltyp[i].append(GraphConvolutionMulti(\n",
        "                input_dim=hidden1_units, output_dim=hidden2_units,\n",
        "                edge_type=(i,j), num_types=self.edge_types[i,j],\n",
        "                adj_mats=self.adj_mats, act=lambda x: x,\n",
        "                dropout=self.dropout, logging=self.logging)(self.hidden1[j]))\n",
        "\n",
        "        self.embeddings = [None] * self.num_obj_types\n",
        "        for i, embeds in self.embeddings_reltyp.items():\n",
        "            # self.embeddings[i] = tf.nn.relu(tf.add_n(embeds))\n",
        "            self.embeddings[i] = tf.add_n(embeds)\n",
        "\n",
        "        self.edge_type2decoder = {}\n",
        "        for i, j in self.edge_types:\n",
        "            decoder = self.decoders[i, j]\n",
        "            if decoder == 'innerproduct':\n",
        "                self.edge_type2decoder[i, j] = InnerProductDecoder(\n",
        "                    input_dim=hidden2_units, logging=self.logging,\n",
        "                    edge_type=(i, j), num_types=self.edge_types[i, j],\n",
        "                    act=lambda x: x, dropout=self.dropout)\n",
        "            else:\n",
        "                raise ValueError('Unknown decoder type')\n",
        "\n",
        "        self.latent_inters = []\n",
        "        self.latent_varies = []\n",
        "        for edge_type in self.edge_types:\n",
        "            decoder = self.decoders[edge_type]\n",
        "            for k in range(self.edge_types[edge_type]):\n",
        "                if decoder == 'innerproduct':\n",
        "                    glb = tf.eye(hidden2_units, hidden2_units)\n",
        "                    loc = tf.eye(hidden2_units, hidden2_units)\n",
        "                else:\n",
        "                    raise ValueError('Unknown decoder type')\n",
        "\n",
        "                self.latent_inters.append(glb)\n",
        "                self.latent_varies.append(loc)\n"
      ],
      "id": "0c9e1e77"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49c6a5d3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Create model\")\n",
        "model = DecagonModel(\n",
        "        placeholders=placeholders,\n",
        "        num_feat=num_feat,\n",
        "        nonzero_feat=nonzero_feat,\n",
        "        edge_types=edge_types,\n",
        "        decoders=edge_type2decoder,\n",
        ")\n"
      ],
      "id": "49c6a5d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYiqz5iLdi-b"
      },
      "source": [
        "# Decagon Optimizer"
      ],
      "id": "eYiqz5iLdi-b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNozH__-2E19"
      },
      "outputs": [],
      "source": [
        "negative_sampler_index_variable = 0"
      ],
      "id": "WNozH__-2E19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fe3c35e"
      },
      "outputs": [],
      "source": [
        "class DecagonOptimizer(object):\n",
        "    def __init__(self, embeddings, latent_inters, latent_varies,\n",
        "                 degrees, edge_types, edge_type2dim, placeholders,\n",
        "                 margin=0.1, neg_sample_weights=1., batch_size=100):\n",
        "        self.embeddings= embeddings\n",
        "        self.latent_inters = latent_inters\n",
        "        self.latent_varies = latent_varies\n",
        "        self.edge_types = edge_types\n",
        "        self.degrees = degrees\n",
        "        self.edge_type2dim = edge_type2dim           #(0, 0): [(12331, 12331), (12331, 12331)] ...\n",
        "        self.obj_type2n = {i: self.edge_type2dim[i,j][0][0] for i, j in self.edge_types}  #{0: 12331, 1: 3215}\n",
        "        self.margin = margin\n",
        "        self.neg_sample_weights = neg_sample_weights\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.inputs = placeholders['batch']\n",
        "        self.batch_edge_type_idx = placeholders['batch_edge_type_idx']\n",
        "        self.batch_row_edge_type = placeholders['batch_row_edge_type']\n",
        "        self.batch_col_edge_type = placeholders['batch_col_edge_type']\n",
        "\n",
        "\n",
        "        self.row_inputs = tf.squeeze(gather_cols(self.inputs, [0]))\n",
        "        self.col_inputs = tf.squeeze(gather_cols(self.inputs, [1]))\n",
        "\n",
        "        obj_type_n = [self.obj_type2n[i] for i in range(len(self.embeddings))]\n",
        "        self.obj_type_lookup_start = tf.cumsum([0] + obj_type_n[:-1])    #0, 12331\n",
        "        self.obj_type_lookup_end = tf.cumsum(obj_type_n)                 #12331, 12331 + 3215\n",
        "\n",
        "        labels = tf.reshape(tf.cast(self.row_inputs, dtype=tf.int64), [self.batch_size, 1])\n",
        "\n",
        "        self.neg_samples_list = []\n",
        "        print_ops = []\n",
        "        for i, j in self.edge_types:\n",
        "            for k in range(self.edge_types[i,j]):\n",
        "                neg_samples, _, _ = tf.nn.fixed_unigram_candidate_sampler(\n",
        "                        true_classes=labels,\n",
        "                        num_true=1,\n",
        "                        num_sampled=self.batch_size,\n",
        "                        unique=False,\n",
        "                        range_max=len(self.degrees[i][k]),\n",
        "                        distortion=0.75,\n",
        "                        unigrams=self.degrees[i][k].tolist())\n",
        "                self.neg_samples_list.append(neg_samples)\n",
        "\n",
        "        print(\"Negative Samples List Created: \", self.neg_samples_list)\n",
        "\n",
        "        print_op = tf.print(\"Index of Negative Sampler: \", negative_sampler_index_variable, \"Negative Samples: \", self.neg_samples_list[negative_sampler_index_variable])\n",
        "        with tf.control_dependencies([print_op]):\n",
        "          self.neg_samples = self.neg_samples_list[negative_sampler_index_variable]\n",
        "        #self.neg_samples_rows = tf.squeeze(gather_cols(self.neg_samples, [0]))\n",
        "\n",
        "        self.preds = self.batch_predict(self.row_inputs, self.col_inputs)   # Diagonol has the product of the row-col embeddings\n",
        "        self.outputs = tf.compat.v1.diag_part(self.preds)\n",
        "        self.outputs = tf.reshape(self.outputs, [-1])\n",
        "\n",
        "        self.neg_preds = self.batch_predict(self.neg_samples, self.col_inputs)\n",
        "        self.neg_outputs = tf.compat.v1.diag_part(self.neg_preds)\n",
        "        self.neg_outputs = tf.reshape(self.neg_outputs, [-1])\n",
        "\n",
        "        self.predict()\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def batch_predict(self, row_inputs, col_inputs):\n",
        "        concatenated = tf.concat(self.embeddings, 0)  # 12331x32 + 3215x32 together along 0 axis\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)            # Only gather embeddings for the partcular type, 1st 12331, or next 3215\n",
        "        row_embeds = tf.gather(row_embeds, row_inputs)         # Takes the 512 embeddings in current batch\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type) # Gather embeddings for Col type\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "        col_embeds = tf.gather(col_embeds, col_inputs)        # Takes the 512 embeddings in current batch\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)  # Identity matrix 32x32\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)     # Identity matrix 32x32\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)   # No effect in Innerproduct for all 3 products\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        preds = tf.matmul(product3, tf.transpose(col_embeds))  #512x32 * 32*512\n",
        "        return preds          # Predition is simply product of two embeddings\n",
        "\n",
        "    def predict(self):\n",
        "        concatenated = tf.concat(self.embeddings, 0)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_row_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_row_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        row_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        ind_start = tf.gather(self.obj_type_lookup_start, self.batch_col_edge_type)\n",
        "        ind_end = tf.gather(self.obj_type_lookup_end, self.batch_col_edge_type)\n",
        "        indices = tf.range(ind_start, ind_end)\n",
        "        col_embeds = tf.gather(concatenated, indices)\n",
        "\n",
        "        latent_inter = tf.gather(self.latent_inters, self.batch_edge_type_idx)\n",
        "        latent_var = tf.gather(self.latent_varies, self.batch_edge_type_idx)\n",
        "\n",
        "        product1 = tf.matmul(row_embeds, latent_var)\n",
        "        product2 = tf.matmul(product1, latent_inter)\n",
        "        product3 = tf.matmul(product2, latent_var)\n",
        "        self.predictions = tf.matmul(product3, tf.transpose(col_embeds))    # Predition is simply product of two embeddings\n",
        "\n",
        "    def _build(self):\n",
        "        self.cost = self._hinge_loss(self.outputs, self.neg_outputs)\n",
        "        #self.cost = self._xent_loss(self.outputs, self.neg_outputs)\n",
        "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "        update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
        "        #grads_and_vars = optim.compute_gradients(total_loss, var_list=train_vars)\n",
        "        #train_op = optim.apply_gradients(grads_and_vars)\n",
        "        self.train_vars = [var for var in tf.compat.v1.trainable_variables()]\n",
        "        print(\"Trainable Variables: \", self.train_vars)\n",
        "        with tf.control_dependencies(update_ops):\n",
        "           self.opt_op = self.optimizer.minimize(self.cost,var_list=self.train_vars)\n",
        "\n",
        "    def _hinge_loss(self, aff, neg_aff):\n",
        "        \"\"\"Maximum-margin optimization using the hinge loss.\"\"\"\n",
        "        diff = tf.nn.relu(tf.subtract(neg_aff, tf.expand_dims(aff, 0) - self.margin), name='diff')\n",
        "        loss = tf.reduce_sum(diff)\n",
        "        return loss\n",
        "\n",
        "    def _xent_loss(self, aff, neg_aff):\n",
        "        \"\"\"Cross-entropy optimization.\"\"\"\n",
        "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(aff), logits=aff)\n",
        "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
        "        loss = tf.reduce_sum(true_xent) + self.neg_sample_weights * tf.reduce_sum(negative_xent)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def gather_cols(params, indices, name=None):\n",
        "    \"\"\"Gather columns of a 2D tensor.\n",
        "\n",
        "    Args:\n",
        "        params: A 2D tensor.\n",
        "        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.\n",
        "        name: A name for the operation (optional).\n",
        "\n",
        "    Returns:\n",
        "        A 2D Tensor. Has the same type as ``params``.\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.op_scope([params, indices], name, \"gather_cols\") as scope:\n",
        "        # Check input\n",
        "        params = tf.convert_to_tensor(params, name=\"params\")\n",
        "        indices = tf.convert_to_tensor(indices, name=\"indices\")\n",
        "        try:\n",
        "            params.get_shape().assert_has_rank(2)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 2D.')\n",
        "        try:\n",
        "            indices.get_shape().assert_has_rank(1)\n",
        "        except ValueError:\n",
        "            raise ValueError('\\'params\\' must be 1D.')\n",
        "\n",
        "        # Define op\n",
        "        p_shape = tf.shape(params)\n",
        "        p_flat = tf.reshape(params, [-1])\n",
        "        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],\n",
        "                                       [-1, 1]) + indices, [-1])\n",
        "        return tf.reshape(\n",
        "            tf.gather(p_flat, i_flat), [p_shape[0], -1])"
      ],
      "id": "1fe3c35e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45657796"
      },
      "outputs": [],
      "source": [
        "print(\"Create optimizer\")\n",
        "with tf.name_scope('optimizer'):\n",
        "        opt = DecagonOptimizer(\n",
        "            embeddings=model.embeddings,\n",
        "            latent_inters=model.latent_inters,\n",
        "            latent_varies=model.latent_varies,\n",
        "            degrees=degrees,\n",
        "            edge_types=edge_types,\n",
        "            edge_type2dim=edge_type2dim,\n",
        "            placeholders=placeholders,\n",
        "            batch_size=batch_size,\n",
        "            margin=max_margin\n",
        "        )"
      ],
      "id": "45657796"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2695d5b"
      },
      "outputs": [],
      "source": [
        "def bedroc_score(y_true, y_pred, decreasing=True, alpha=20.0):\n",
        "\n",
        "    \"\"\"BEDROC metric implemented according to Truchon and Bayley.\n",
        "\n",
        "    The Boltzmann Enhanced Descrimination of the Receiver Operator\n",
        "    Characteristic (BEDROC) score is a modification of the Receiver Operator\n",
        "    Characteristic (ROC) score that allows for a factor of *early recognition*.\n",
        "\n",
        "    References:\n",
        "        The original paper by Truchon et al. is located at `10.1021/ci600426e\n",
        "        <http://dx.doi.org/10.1021/ci600426e>`_.\n",
        "\n",
        "    Args:\n",
        "        y_true (array_like):\n",
        "            Binary class labels. 1 for positive class, 0 otherwise.\n",
        "        y_pred (array_like):\n",
        "            Prediction values.\n",
        "        decreasing (bool):\n",
        "            True if high values of ``y_pred`` correlates to positive class.\n",
        "        alpha (float):\n",
        "            Early recognition parameter.\n",
        "\n",
        "    Returns:\n",
        "        float:\n",
        "            Value in interval [0, 1] indicating degree to which the predictive\n",
        "            technique employed detects (early) the positive class.\n",
        "     \"\"\"\n",
        "\n",
        "    assert len(y_true) == len(y_pred), \\\n",
        "        'The number of scores must be equal to the number of labels'\n",
        "\n",
        "    big_n = len(y_true)\n",
        "    n = sum(y_true == 1)\n",
        "\n",
        "    if decreasing:\n",
        "        order = np.argsort(-y_pred)\n",
        "    else:\n",
        "        order = np.argsort(y_pred)\n",
        "\n",
        "    m_rank = (y_true[order] == 1).nonzero()[0]\n",
        "\n",
        "    s = np.sum(np.exp(-alpha * m_rank / big_n))\n",
        "\n",
        "    r_a = n / big_n\n",
        "\n",
        "    rand_sum = r_a * (1 - np.exp(-alpha))/(np.exp(alpha/big_n) - 1)\n",
        "\n",
        "    fac = r_a * np.sinh(alpha / 2) / (np.cosh(alpha / 2) -\n",
        "                                      np.cosh(alpha/2 - alpha * r_a))\n",
        "\n",
        "    cte = 1 / (1 - np.exp(alpha * (1 - r_a)))\n",
        "\n",
        "    return s * fac / rand_sum + cte\n",
        "\n"
      ],
      "id": "f2695d5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58fa5732"
      },
      "outputs": [],
      "source": [
        "def apk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the average precision at k.\n",
        "\n",
        "    This function computes the average precision at k between two lists of\n",
        "    items.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of elements that are to be predicted (order doesn't matter)\n",
        "    predicted : list\n",
        "                A list of predicted elements (order does matter)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The average precision at k over the input lists\n",
        "\n",
        "    \"\"\"\n",
        "    if len(predicted)>k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "\n",
        "    for i, p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "\n",
        "    if not actual:\n",
        "        return 0.0\n",
        "\n",
        "    return score / min(len(actual), k)\n",
        "\n",
        "\n",
        "def ark(actual, predicted, k=10):\n",
        "\n",
        "    if len(predicted)>k:\n",
        "        predicted = predicted[:k]\n",
        "\n",
        "    num_actual = len(actual)\n",
        "\n",
        "    num_hits = 0.0\n",
        "    if len(actual)==0:\n",
        "        return 0\n",
        "\n",
        "    for i, p in enumerate(actual):\n",
        "        if p in predicted:\n",
        "            num_hits += 1.0\n",
        "\n",
        "\n",
        "    return num_hits / len(actual)\n",
        "\n",
        "def mapk(actual, predicted, k=10):\n",
        "    \"\"\"\n",
        "    Computes the mean average precision at k.\n",
        "\n",
        "    This function computes the mean average precision at k between two lists\n",
        "    of lists of items.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list\n",
        "             A list of lists of elements that are to be predicted\n",
        "             (order doesn't matter in the lists)\n",
        "    predicted : list\n",
        "                A list of lists of predicted elements\n",
        "                (order matters in the lists)\n",
        "    k : int, optional\n",
        "        The maximum number of predicted elements\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The mean average precision at k over the input lists\n",
        "\n",
        "    \"\"\"\n",
        "    return np.mean([apk(a,p,k) for a, p in zip(actual, predicted)])\n"
      ],
      "id": "58fa5732"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2166886"
      },
      "outputs": [],
      "source": [
        "def get_accuracy_scores(edges_pos, edges_neg, edge_type, name=None):\n",
        "\n",
        "    negative_sampler_index_variable = 3\n",
        "    feed_dict.update({placeholders['dropout']: 0})\n",
        "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
        "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
        "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
        "    fetches = {\n",
        "            'predictions': opt.predictions,\n",
        "    }\n",
        "\n",
        "    res = sess.run(fetches, feed_dict=feed_dict)\n",
        "    rec = res['predictions']\n",
        "\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1. / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "\n",
        "    preds = []\n",
        "    actual = []\n",
        "    predicted = []\n",
        "    edge_ind = 0\n",
        "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds.append(score)\n",
        "\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
        "\n",
        "        actual.append(edge_ind)\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_neg = []\n",
        "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        preds_neg.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
        "\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    all_edge_idx = list(range(minibatch.train_edges[edge_type[:2]][edge_type[2]].shape[0]))\n",
        "    np.random.shuffle(all_edge_idx)\n",
        "    train_edge_idx = all_edge_idx[:100]\n",
        "    train_preds = []\n",
        "    for u, v in minibatch.train_edges[edge_type[:2]][edge_type[2]][train_edge_idx]:\n",
        "        score = sigmoid(rec[u, v])\n",
        "        train_preds.append(score)\n",
        "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 0'\n",
        "\n",
        "        predicted.append((score, edge_ind))\n",
        "        edge_ind += 1\n",
        "\n",
        "    preds_mean = np.mean(preds)\n",
        "    neg_preds_mean = np.mean(preds_neg)\n",
        "    train_mean = np.mean(train_preds)\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "    preds_all = np.nan_to_num(preds_all)\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
        "\n",
        "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
        "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
        "    apk_sc = apk(actual, predicted, k=200)\n",
        "    bedroc_sc = bedroc_score(labels_all, preds_all)\n",
        "    if name!=None:\n",
        "        with open(name, 'wb') as f:\n",
        "            pickle.dump([labels_all, preds_all], f)\n",
        "    return roc_sc, aupr_sc, apk_sc, bedroc_sc, preds_mean, neg_preds_mean, train_mean"
      ],
      "id": "f2166886"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c47f7dd"
      },
      "outputs": [],
      "source": [
        "def get_prediction(edges_pos, edges_neg, edge_type):\n",
        "    feed_dict.update({placeholders['dropout']: 0})\n",
        "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
        "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
        "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
        "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
        "\n",
        "    return 1. / (1 + np.exp(-rec))"
      ],
      "id": "1c47f7dd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzsrShmIX8VT"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.global_variables()"
      ],
      "id": "YzsrShmIX8VT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0874af6"
      },
      "outputs": [],
      "source": [
        "print(\"Initialize session\")\n",
        "checkpoint_dir = '/content/drive/MyDrive/PGCN/model/checkpoints/'\n",
        "saver = tf.compat.v1.train.Saver([var for var in tf.compat.v1.global_variables()], max_to_keep=10)\n",
        "sess = tf.compat.v1.Session()\n",
        "\n",
        "feed_dict = {}\n",
        "\n",
        "checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "if checkpoint is not None:\n",
        "  print(\"Restoring checkpoint from {}\".format(checkpoint))\n",
        "  saver.restore(sess, checkpoint)\n",
        "else:\n",
        "  print(\"Initializing variables\")\n",
        "  sess.run(tf.compat.v1.global_variables_initializer())\n",
        "#grads_and_vars = opt.optimizer.compute_gradients(opt.cost, var_list=train_vars)\n",
        "#train_op = opt.optimizer.apply_gradients(grads_and_vars)\n"
      ],
      "id": "e0874af6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz9-mfNRRnie"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  epoch_losses_history = np.ndarray.tolist(np.load(os.path.join(checkpoint_dir, 'epoch_losses.npy'), allow_pickle=True))\n",
        "  print(\"Previous epoch losses:\", epoch_losses_history)\n",
        "except:\n",
        "  epoch_losses_history = defaultdict(list)\n",
        "\n",
        "try:\n",
        "  minibatch.batch_num = np.ndarray.tolist(np.load(os.path.join(checkpoint_dir, 'batch_numbers.npy'), allow_pickle=True))\n",
        "  print(\"Previous batch Numbers:\", minibatch.batch_num)\n",
        "except:\n",
        "  minibatch.batch_num = [0]*minibatch.num_edge_types"
      ],
      "id": "rz9-mfNRRnie"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3012e297",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "for step in range(0, 30000):\n",
        "    negative_sampler_index_variable = step%6\n",
        "    feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
        "    feed_dict = minibatch.update_feed_dict(\n",
        "        feed_dict=feed_dict,\n",
        "        dropout=dropout,\n",
        "        placeholders=placeholders)\n",
        "\n",
        "\n",
        "    fetches = {\n",
        "          'loss': opt.cost,\n",
        "          'train': opt.opt_op\n",
        "        }\n",
        "\n",
        "\n",
        "    results = sess.run(fetches, feed_dict = feed_dict)\n",
        "    #print(\"Loss:\", results['loss'])\n",
        "    current_epoch_loss[minibatch.current_edge_type_idx].append(results['loss'])\n",
        "\n",
        "    if(step%100 == 1):\n",
        "        clear_output()\n",
        "        saver.save(sess, os.path.join(checkpoint_dir, 'model.latest'))\n",
        "        np.save(os.path.join(checkpoint_dir, 'epoch_losses.npy'), epoch_losses_history)\n",
        "        np.save(os.path.join(checkpoint_dir, 'batch_numbers.npy'), minibatch.batch_num)\n",
        "        for i in [3]:\n",
        "            edge_type = minibatch.idx2edge_type[i]\n",
        "            roc_score, auprc_score, apk_score, bedroc, val_preds, val_negs, train_preds = get_accuracy_scores(\n",
        "                minibatch.val_edges, minibatch.val_edges_false, minibatch.idx2edge_type[i])\n",
        "            print(\"Edge type=\", \"[%02d, %02d, %02d]\" % minibatch.idx2edge_type[i])\n",
        "            print(\"Edge type:\", \"%04d\" % 3, \"Val Preds Avg\", \"{:.5f}\".format(val_preds))\n",
        "            print(\"Edge type:\", \"%04d\" % 3, \"Val Neg Avg\", \"{:.5f}\".format(val_negs))\n",
        "            print(\"Edge type:\", \"%04d\" % 3, \"Val AUROC score\", \"{:.5f}\".format(roc_score))\n",
        "            print(\"Edge type:\", \"%04d\" % 3, \"Val AUPRC score\", \"{:.5f}\".format(auprc_score))\n",
        "            print(\"Edge type:\", \"%04d\" % 3, \"Val AP@k score\", \"{:.5f}\".format(apk_score))\n",
        "            print(\"Edge type:\", \"%04d\" % 3, \"Val BEDROC score\", \"{:.5f}\".format(bedroc))\n",
        "            print(\"All Edge Average Losses\", epoch_losses_history)\n",
        "            print()\n",
        "\n",
        "\n",
        "\n",
        "prediction = get_prediction(minibatch.test_edges, minibatch.test_edges_false,\n",
        "    \tminibatch.idx2edge_type[3])\n",
        "\n",
        "print('Saving result...')\n",
        "np.save(os.path.join(checkpoint_dir,'prediction.npy'), prediction)"
      ],
      "id": "3012e297"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uP8ufxSzc-h7"
      },
      "id": "uP8ufxSzc-h7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81a16d76"
      },
      "outputs": [],
      "source": [
        "minibatch.train_edges[0,0][0].size"
      ],
      "id": "81a16d76"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}